{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817219cc-44af-45b1-ad8e-aa2994da5ac6",
   "metadata": {},
   "source": [
    "# Web-to-Graph Pipeline\n",
    "\n",
    "This Jupyter Notebook will guide you step-by-step on how to build a web-powered knowledge graph and automate it with Prefect. \n",
    "\n",
    "### Step 1: Extract data with Wikipedia API\n",
    "\n",
    "Install Required Packages\n",
    "```bash\n",
    "pip install requests transformers neo4j python-dotenv prefect pyvis openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51135291-e75b-45e2-a6dd-2b5ab42faeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of CEO names so Wikipedia knows which ones to return. \n",
    "\n",
    "ceo_names = [\n",
    "    \"Elon Musk\", \"Sundar Pichai\", \"Tim Cook\", \"Satya Nadella\", \"Mark Zuckerberg\",\n",
    "    \"Andy Jassy\", \"Jensen Huang\", \"Ginni Rometty\", \"Larry Page\", \"Susan Wojcicki\",\n",
    "    \"Shantanu Narayen\", \"Reed Hastings\", \"Michael Dell\", \"Daniel Ek\", \"Evan Spiegel\",\n",
    "    \"Marc Benioff\", \"Lisa Su\", \"Dara Khosrowshahi\", \"Patrick Collison\", \"Brian Chesky\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5bf716-ae3f-4bcb-9f2a-0ab90174ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves the Wikipedia summaries of the highlighted CEOs.\n",
    "\n",
    "import requests\n",
    "def get_wikipedia_summary(name):\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"extract\", \"\")\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d3fc9-6bdf-4896-b3aa-fba7b697ee56",
   "metadata": {},
   "source": [
    "Wikipedia API outputs structured data which removes the need for manual preprocessing\n",
    "\n",
    "### Step 2: Entity recognition and relationship mapping using dslim/bert-base-NER model\n",
    "We extract PER, ORG and LOC entities from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01bf85-2b09-411b-94f3-3d578709990f",
   "metadata": {},
   "source": [
    "#Initialize the NER pipeline.\n",
    "from transformers import pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500a1a1-c759-461a-966e-b93a1341c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract organization (ORG) entity from each CEO summary\n",
    "def extract_organizations(text):\n",
    "    ner_results = ner_pipeline(text)\n",
    "    orgs = set()\n",
    "    for entity in ner_results:\n",
    "        if entity[\"entity_group\"] == \"ORG\":\n",
    "            orgs.add(entity[\"word\"])\n",
    "    return list(orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5342bb-7449-4a91-a0f9-02940b75f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each CEO to their company\n",
    "\n",
    "ceo_to_companies = {}\n",
    "for ceo in ceo_names:\n",
    "    summary = get_wikipedia_summary(ceo)\n",
    "    organizations = extract_organizations(summary)\n",
    "    ceo_to_companies[ceo] = organizations\n",
    "\n",
    "ceo_to_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8c56e-56f7-4b18-8adb-a6c24f17953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all unique company entities to remove any duplicates using .update() set method. \n",
    "\n",
    "all_companies = set()\n",
    "for company_list in ceo_to_companies.values():\n",
    "    all_companies.update(company_list)\n",
    "\n",
    "all_companies = list(all_companies)\n",
    "print(\"Unique companies found:\", all_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99eec48-0af2-4e49-978c-b05bfc5ff5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract location (LOC) entity.\n",
    "\n",
    "def extract_locations(text):\n",
    "    ner_results = ner_pipeline(text)\n",
    "    locations = set()\n",
    "    for entity in ner_results:\n",
    "# Check if the entity is labeled as a location\n",
    "        if entity[\"entity_group\"] == \"LOC\":\n",
    "            locations.add(entity[\"word\"])\n",
    "    return list(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28555776-c8fa-450b-aa99-8195e4325d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the relationship between location and company.\n",
    "\n",
    "company_to_locations = {}\n",
    "for company in all_companies:\n",
    "    summary = get_wikipedia_summary(company)\n",
    "    locations = extract_locations(summary)\n",
    "    company_to_locations[company] = locations\n",
    "\n",
    "company_to_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83199119-0005-4730-a551-5276e10829b4",
   "metadata": {},
   "source": [
    "### Step 3: Store the data in Neo4j\n",
    "Set up an instance in Neo4j AuraDB and save the connection URI, username (typically, Neo4j) and password as environment variables using a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07d8681e-adf2-444b-b03f-cc087bf89ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "uri = os.environ[\"NEO4J_URI\"]\n",
    "user = os.environ[\"NEO4J_USERNAME\"]\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298e4d7-fe12-4504-8b19-6132ddb14449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate Neo4j with the company info. \n",
    "from datetime import datetime\n",
    "def create_node_with_provenance(session, label, name, timestamp):\n",
    "    session.run(f\"\"\"\n",
    "        MERGE (n:{label} {{name: $name}})\n",
    "        WITH n\n",
    "        MATCH (p:Provenance {{run_time: datetime($time)}})\n",
    "        MERGE (n)-[:EXTRACTED_FROM]->(p)\n",
    "    \"\"\", name=name, time=timestamp)\n",
    "def create_knowledge_graph(ceo_to_companies, company_to_locations):\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "    source = \"https://en.wikipedia.org/api/rest_v1/page/summary\"\n",
    "    with driver.session() as session:\n",
    "        #provenance node for the entire data load\n",
    "        session.run(\"\"\"\n",
    "            MERGE (p:Provenance {run_time: datetime($time)})\n",
    "            SET p.source = $source\n",
    "        \"\"\", time=timestamp, source=source)\n",
    "        for ceo, companies in ceo_to_companies.items():\n",
    "            create_node_with_provenance(session, \"CEO\", ceo, timestamp)\n",
    "            for company in companies:\n",
    "                create_node_with_provenance(session, \"Company\", company, timestamp)\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (c:CEO {name:$ceo}), (comp:Company {name:$company})\n",
    "                    MERGE (c)-[:OWNS]->(comp)\n",
    "                \"\"\", ceo=ceo, company=company)\n",
    "                for location in company_to_locations.get(company, []):\n",
    "                    create_node_with_provenance(session, \"Location\", location, timestamp)\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (comp:Company {name:$company}), (l:Location {name:$location})\n",
    "                        MERGE (comp)-[:LOCATED_IN]->(l)\n",
    "                    \"\"\", company=company, location=location)\n",
    "create_knowledge_graph(ceo_to_companies, company_to_locations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117a9ca-9888-43f5-b84d-2e8cb5c84921",
   "metadata": {},
   "source": [
    "This also creates a provenance entity which has `source` and `run_time` properties, and connects to other existing entities. Provenance is crucial for the reproducibility and traceability of knowledge graphs because it provides metadata about the data origin, retrieval time and extracted entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1572119-921b-4a8d-a893-85aacc3fb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view provenance\n",
    "\n",
    "def view_provenance():\n",
    "    with driver.session() as session:\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Provenance)<-[:EXTRACTED_FROM]-(n)\n",
    "        RETURN p.source AS source, p.run_time AS timestamp, collect(n.name) AS linked_entities\n",
    "        \"\"\"\n",
    "        result = session.run(query)\n",
    "        for record in result:\n",
    "            print(f\"Source: {record['source']}\")\n",
    "            print(f\"Timestamp: {record['timestamp']}\")\n",
    "            print(\"Entities:\", record['linked_entities'])\n",
    "view_provenance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d42eef-2f95-4f88-8c6e-61f06244d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns knowledge from the graph. \n",
    "\n",
    "\n",
    "def get_graph_data():\n",
    "    query = \"\"\"\n",
    "    MATCH (ceo:CEO)-[:OWNS]->(company:Company)\n",
    "    OPTIONAL MATCH (company)-[:LOCATED_IN]->(loc:Location)\n",
    "    RETURN ceo.name AS ceo, company.name AS company, loc.name AS location\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        results = session.run(query)\n",
    "        return [record.data() for record in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a4147-67b7-420b-bfbf-7888bcb5fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "\n",
    "\n",
    "from pyvis.network import Network\n",
    "def visualize_graph(data):\n",
    "    net = Network(height='600px', width='100%', notebook=True)\n",
    "    for item in data:\n",
    "        ceo = item['ceo']\n",
    "        company = item['company']\n",
    "        location = item['location']\n",
    "        net.add_node(ceo, label=ceo, color='orange', shape='dot')\n",
    "        net.add_node(company, label=company, color='lightblue', shape='box')\n",
    "        net.add_edge(ceo, company, label='OWNS')\n",
    "        if location:\n",
    "            net.add_node(location, label=location, color='lightgreen', shape='ellipse')\n",
    "            net.add_edge(company, location, label='LOCATED_IN')\n",
    "    net.show('ceo_graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99571bf-aadf-4b66-9206-068eb8adf057",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_graph_data()\n",
    "visualize_graph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1456845-3c39-45c1-bf5d-8f4810bd7402",
   "metadata": {},
   "source": [
    "### Step 4: Query and validate the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16e4fa-273f-470e-9af2-d1f15443e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import openAI\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd2493-43ba-411c-8616-7f32c197abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your API token from OpenAI and set it as an environment variable.\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781512f-4bda-47e8-a15e-b706ff323ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Cypher query with GPT-4.1 to query the Neo4j\n",
    "\n",
    "def generate_cypher_query(user_input):\n",
    "# Define the prompt that guides gpt-4.1 to translate natural language into Cypher\n",
    "    prompt = f\"\"\"\n",
    "You are an assistant that translates natural language into Cypher queries for a Neo4j graph.\n",
    "The graph has the following entities:\n",
    "- CEO (with `name` property)\n",
    "- Company (with `name` property)\n",
    "- Location (with `name` property)\n",
    "Relationships:\n",
    "- (ceo:CEO)-[:OWNS]->(company:Company)\n",
    "- (company:Company)-[:LOCATED_IN]->(location:Location)\n",
    "Now write a Cypher query for the following request:\n",
    "\"{user_input}\"\n",
    "Only return the query, without explanations.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dd2cd-4db1-4562-8e59-91afee7ea1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Cypher query\n",
    "\n",
    "def execute_cypher_query(cypher_query):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(cypher_query)\n",
    "        return [record.data() for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe78487-b96d-4c05-bb9d-28426cb2c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_graph_with_natural_output(user_input):\n",
    "    print(\"Step 1: Generating Cypher query...\")\n",
    "    cypher_query = generate_cypher_query(user_input)\n",
    "    print(\"Cypher:\", cypher_query)\n",
    "    print(\"Step 2: Executing on Neo4j...\")\n",
    "    results = execute_cypher_query(cypher_query)\n",
    "    print(\"Raw results:\", results)\n",
    "    print(\"Step 3: Generating human-friendly response...\")\n",
    "    \n",
    "    response_prompt = f\"\"\"\n",
    "You are an assistant that explains the result of a database query to users in plain English.\n",
    "User Question:\n",
    "\"{user_input}\"\n",
    "Query Results:\n",
    "{results}\n",
    "Write a clear and concise answer to the user's question based on the results.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": response_prompt}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f81ceb-ada8-476a-8b3d-695148e835e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_graph_with_natural_output(\"Which company does Satya Nadella own?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d76f6-be87-4abc-aa5e-28745b9a6866",
   "metadata": {},
   "source": [
    "### Step 5: Automate data ingestion using Prefect\n",
    "\n",
    "You can either run Prefect on a self-hosted server or in Prefect Cloud. For this graph, we demonstrate how to set up Prefect and a CRON job in your local server to provide a steady flow of fresh data from Wikipedia. We automate three stages:\n",
    "\n",
    "1. Data extraction\n",
    "2. NER processing\n",
    "3. Data loading into Neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa2716-c512-4343-8a56-6ce4764f445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor the existing code with Prefect\n",
    "\n",
    "from prefect import task, flow, get_run_logger\n",
    "from datetime import datetime, timezone\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "\n",
    "\n",
    "def get_wikipedia_summary(name):\n",
    "    \"\"\"Fetch summary text from Wikipedia API.\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"extract\", \"\")\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_organizations(text, ner_pipeline):\n",
    "    \"\"\"Extract organizations from text using NER model.\"\"\"\n",
    "    orgs = set()\n",
    "    for entity in ner_pipeline(text):\n",
    "        if entity[\"entity_group\"] == \"ORG\":\n",
    "            orgs.add(entity[\"word\"])\n",
    "    return list(orgs)\n",
    "\n",
    "def extract_locations(text, ner_pipeline):\n",
    "    \"\"\"Extract locations from text using NER model.\"\"\"\n",
    "    locations = set()\n",
    "    for entity in ner_pipeline(text):\n",
    "        if entity[\"entity_group\"] == \"LOC\":\n",
    "            locations.add(entity[\"word\"])\n",
    "    return list(locations)\n",
    "\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=10)\n",
    "def create_knowledge_graph(ceo_to_companies, company_to_locations):\n",
    "    \"\"\"data in Neo4j with provenance tracking.\"\"\"\n",
    "    uri = os.environ[\"NEO4J_URI\"]\n",
    "    user = os.environ[\"NEO4J_USERNAME\"]\n",
    "    password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "    source = \"https://en.wikipedia.org/api/rest_v1/page/summary\"\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Create a provenance node for this entire run\n",
    "        session.run(\"\"\"\n",
    "            MERGE (p:Provenance {run_time: datetime($time)})\n",
    "            SET p.source = $source\n",
    "        \"\"\", time=timestamp, source=source)\n",
    "\n",
    "        for ceo, companies in ceo_to_companies.items():\n",
    "            session.run(\"\"\"\n",
    "                MERGE (c:CEO {name: $ceo})\n",
    "                WITH c\n",
    "                MATCH (p:Provenance {run_time: datetime($time)})\n",
    "                MERGE (c)-[:EXTRACTED_FROM]->(p)\n",
    "            \"\"\", ceo=ceo, time=timestamp)\n",
    "\n",
    "            for company in companies:\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (comp:Company {name: $company})\n",
    "                    WITH comp\n",
    "                    MATCH (p:Provenance {run_time: datetime($time)})\n",
    "                    MERGE (comp)-[:EXTRACTED_FROM]->(p)\n",
    "                \"\"\", company=company, time=timestamp)\n",
    "\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (c:CEO {name:$ceo}), (comp:Company {name:$company})\n",
    "                    MERGE (c)-[:OWNS]->(comp)\n",
    "                \"\"\", ceo=ceo, company=company)\n",
    "\n",
    "                for location in company_to_locations.get(company, []):\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (l:Location {name: $location})\n",
    "                        WITH l\n",
    "                        MATCH (p:Provenance {run_time: datetime($time)})\n",
    "                        MERGE (l)-[:EXTRACTED_FROM]->(p)\n",
    "                    \"\"\", location=location, time=timestamp)\n",
    "\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (comp:Company {name:$company}), (l:Location {name:$location})\n",
    "                        MERGE (comp)-[:LOCATED_IN]->(l)\n",
    "                    \"\"\", company=company, location=location)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "# Prefect Flow\n",
    "\n",
    "@flow(name=\"knowledge-graph-pipeline\")\n",
    "def knowledge_graph_pipeline(ceo_names, ner_pipeline):\n",
    "    \"\"\"Main flow to build and update the Knowledge Graph.\"\"\"\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    # Extract companies for each CEO\n",
    "    ceo_to_companies = {}\n",
    "    for ceo in ceo_names:\n",
    "        summary = get_wikipedia_summary(ceo)\n",
    "        companies = extract_organizations(summary, ner_pipeline)\n",
    "        ceo_to_companies[ceo] = companies\n",
    "\n",
    "    # Collect unique companies\n",
    "    all_companies = set()\n",
    "    for company_list in ceo_to_companies.values():\n",
    "        all_companies.update(company_list)\n",
    "\n",
    "    # Extract locations for each company\n",
    "    company_to_locations = {}\n",
    "    for company in all_companies:\n",
    "        summary = get_wikipedia_summary(company)\n",
    "        locations = extract_locations(summary, ner_pipeline)\n",
    "        company_to_locations[company] = locations\n",
    "\n",
    "    # Load graph into Neo4j\n",
    "    create_knowledge_graph(ceo_to_companies, company_to_locations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e878680-2a85-4d71-80d5-2a31878f2871",
   "metadata": {},
   "source": [
    "Save the script as a `.py`  file and run locally in your terminal to test. \n",
    "\n",
    "PERFORM THE FOLLOWING STEPS IN YOUR TERMINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d7215-9cca-4e12-bddd-b400bbca7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide file path\n",
    "python kg_pipeline.py #file was saved as kg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4186417-bf8e-4b94-81e7-836e128fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Prefect deployment with a CRON schedule. \n",
    "\n",
    "prefect work-pool create \"default\"\n",
    "\n",
    "# This creates a process-based work pool, meaning Prefect will run your flow on your local machine.\n",
    "\n",
    "prefect deploy --name \"kg_pipeline\" --cron \"0 6 * * *\" --pool \"Knowledge graph\" kg_pipeline.py:knowledge_graph_pipeline #sets schedule to 6am UTC daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0401d13-1779-4d61-bce3-d96a23a3ced4",
   "metadata": {},
   "source": [
    "The deployment configuration will be saved as a `.yaml` file. You can make changes to the configuration by modifying the `.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02ec96-0709-4e4f-b66a-9b2dbb316fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CRON schedule.\n",
    "\n",
    "prefect deployment inspect knowledge-graph-pipeline/kg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68487bcb-ea08-4e50-afd9-e335ff12ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a local Prefect server\n",
    "\n",
    "prefect server start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1063b-7991-42aa-ab78-b9b647ba8b11",
   "metadata": {},
   "source": [
    "Keep this server running in a separate terminal window. It acts as the backend where deployments are registered, schedules are stored and Prefect Workers (the component responsible for running the flows) connect to fetch tasks. If successful, you will see this:\n",
    "\n",
    "\n",
    "`PREFECT_API_URL=http://127.0.0.1:4200/api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fc45e-3b81-4fe4-83c8-854ca450e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Prefect Worker\n",
    "\n",
    "prefect worker start -p \"Knowledge graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9170ba-c8b6-4f40-bec6-cc12588f7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should connect successfully to your local server. You can confirm the connection using:\n",
    "\n",
    "prefect work-pool inspect \"Knowledge graph\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (AI Env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
